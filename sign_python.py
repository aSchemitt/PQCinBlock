import pandas as pd
import numpy as np
from pathlib import Path

# Internal imports
import utils
import save
import logging 
import sys

def _run_selected_variants(variants_by_module, functions, runs=5, warm_up=1):
    results = []

    # Inverte o dicionÃ¡rio {module: variants} => {algorithm: module}
    variant_to_module = {
        algorithm: module
        for module, variants in variants_by_module.items()
        for algorithm in variants
    }
    
    for algorithm, module in variant_to_module.items():
        logging.info("")
        logging.info(f"\tAlgorithm: {algorithm}")

        if module not in functions:
            logging.info(f"No function available for '{algorithm}'.")
            continue

        func = functions[module]
        levels = variants_by_module[module][algorithm]

        for level, variant in levels.items():
            try:
                logging.info(f"\t\tRunning algorithm:{algorithm}\tlevel:{level}\tvariant:{variant}  (using {module}.py)...")
                # Warm-up
                func(variant, runs=warm_up)
                # Runs
                df = func(variant, runs=runs)
                df["algorithm"] = algorithm
                df["level"] = level
                results.append(df)

            except Exception as e:
                logging.error(f"\t\tError: {module}/{variant}: {e}")
                sys.exit(-1)

    return pd.concat(results, ignore_index=True) if results else pd.DataFrame()

def print_by_variants(filtered_algorithms: dict):
    for module, algorithms in filtered_algorithms.items():
        for mechanism, variants in algorithms.items():
            logging.info(f"\t{mechanism} - begin")
            for level, variant in sorted(variants.items()):
                logging.info(f"\t\t{' ' * 4}{variant} - NIST Level {level}")
            logging.info(f"\t{mechanism} - end")
            logging.info()

def executions(
    levels,
    variants_by_module,
    evaluations_functions,
    runs, 
    warm_up
):

    df_time_evaluation = _run_selected_variants(
        variants_by_module=variants_by_module, 
        functions=evaluations_functions,
        runs=runs,
        warm_up=warm_up,
    )

    df_time_evaluation_mean_std = utils.compute_mean_std(
        df=df_time_evaluation, 
        group_by="variant",
        columns=[
            "keypair",
            "sign",
            "verify"
        ]
    )

    dfs = {
        f"time-evaluation-{runs}x": df_time_evaluation,
        "time-evaluation-mean-std": df_time_evaluation_mean_std,
    }
    logging.info("")
    logging.info("Files generated by benchmarking:")

    dir_results, algorithms_runs_directory = save.save_results_algorithm_runs(
        dfs=dfs,
        algorithms_dict=variants_by_module,
        levels=levels
    )
    
    path_csv = algorithms_runs_directory / "time-evaluation-mean-std.csv"

    return dir_results, str(path_csv)
