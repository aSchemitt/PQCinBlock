import os
from pathlib import Path
import pandas as pd
import numpy as np
from time import perf_counter

# Internal imports
import utils
import save
import logging 
import sys

def print_by_variants(filtered_algorithms: dict):
    for module, algorithms in filtered_algorithms.items():
        for mechanism, variants in algorithms.items():
            logging.info(f"\t{mechanism} - begin")
            for level, variant in sorted(variants.items()):
                logging.info(f"\t\t{' ' * 4}{variant} - NIST Level {level}")
            logging.info(f"\t{mechanism} - end")
            logging.info()


def _run_selected_variants(file_runs, file_mean_std, variants_by_module, function, runs=5, warm_up=1):

    # Inverte o dicionÃ¡rio {module: variants} => {algorithm: module}
    variant_to_module = {
        algorithm: module
        for module, variants in variants_by_module.items()
        for algorithm in variants
    }
    
    for algorithm, module in variant_to_module.items():
        logging.info("")
        logging.info(f"\tAlgorithm: {algorithm}")

        if module not in function:
            logging.info(f"No function available for '{algorithm}'.")
            continue

        func = function[module]
        levels = variants_by_module[module][algorithm]

        for level, variant in levels.items():
            try:
                logging.info(f"\t\tRunning algorithm:{algorithm}\tlevel:{level}\tvariant:{variant}  (using {module}.py)...")

                # Warm-up
                func(variant, runs=warm_up)
                
                # Runs
                start = perf_counter()
                variant_runs = func(variant, runs=runs)
                end = perf_counter()
                df_time_evaluation = pd.DataFrame(variant_runs)
                df_time_evaluation["algorithm"] = algorithm
                df_time_evaluation["level"] = level

                # Save results of this run
                write_header = not os.path.exists(file_runs)
                df_time_evaluation.to_csv(file_runs, mode='a', header=write_header, index=False)

                df_mean_std = utils.compute_mean_std(
                    df=df_time_evaluation, 
                    group_by="variant",
                    columns=["keypair","sign","verify"]
                )
                
                # Save average and std results for this run
                write_header = not os.path.exists(file_mean_std)
                df_mean_std.to_csv(file_mean_std, mode='a', header=write_header, index=False)

                # Get the current variant
                row = df_mean_std[df_mean_std["variant"] == variant].iloc[0]
                
                logging.info(f"\t\t\tTotal duration time: {(end - start):.6f} s")

                mean_label_width = 12
                std_label_width = 11
                logging.info(f"\t\t\t{'mean keypair':<{mean_label_width}} = {row['mean_keypair']:.6f} ms\t{'std keypair':<{std_label_width}} = {row['std_keypair']:.6f} ms")
                logging.info(f"\t\t\t{'mean sign':<{mean_label_width}} = {row['mean_sign']:.6f} ms\t{'std keypair':<{std_label_width}} = {row['std_sign']:.6f} ms")
                logging.info(f"\t\t\t{'mean verify':<{mean_label_width}} = {row['mean_verify']:.6f} ms\t{'std keypair':<{std_label_width}} = {row['std_verify']:.6f} ms")
            
            except Exception as e:
                logging.error(f"\t\tError: {module}/{variant}: {e}")
                sys.exit(-1)


def benchmark(
    dir_results,
    levels,
    variants_by_module,
    evaluation_function,
    runs, 
    warm_up
):

    dir_benchmark = save.create_benchmark_directory(dir_results)
    path_csv_runs = dir_benchmark / f"time-evaluation-{runs}x.csv"
    path_csv_mean_std = dir_benchmark / f"time-evaluation-mean-std.csv"

    df_time_evaluation = _run_selected_variants(
        file_runs=path_csv_runs,
        file_mean_std=path_csv_mean_std,
        variants_by_module=variants_by_module, 
        function=evaluation_function,
        runs=runs,
        warm_up=warm_up,
    )

    logging.info("")
    logging.info("Files generated by benchmarking:")
    logging.info(f"\t{path_csv_runs}")
    logging.info(f"\t{path_csv_mean_std}")

    return path_csv_mean_std
